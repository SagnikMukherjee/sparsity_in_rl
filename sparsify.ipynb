{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagnikm3/miniconda3/envs/verl_2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 99.45it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 40.99it/s]\n"
     ]
    }
   ],
   "source": [
    "SFT_MODEL = \"meta-llama/Llama-3.1-8B\"\n",
    "RL_MODEL  = \"/shared/storage-01/users/sagnikm3/tulu_SFT_2000_steps_lr_5e-7\"\n",
    "\n",
    "# SFT_MODEL = \"/home/sagnikm3/verl/checkpoints/prime_example/Eurus-2-7B-SFT-gsm8k/global_step_320/actor/hf/\"\n",
    "# RL_MODEL  = \"/home/sagnikm3/verl/checkpoints/prime_example/Eurus-2-7B-SFT-gsm8k_masked/global_step_320/actor/hf\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL)\n",
    "\n",
    "model_sft = AutoModelForCausalLM.from_pretrained(\n",
    "    SFT_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cpu\",\n",
    "    cache_dir=\"/shared/storage-01/huggingface/models/\"\n",
    ")\n",
    "model_rl = AutoModelForCausalLM.from_pretrained(\n",
    "    RL_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cpu\",\n",
    "    cache_dir=\"/shared/storage-01/huggingface/models/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 139.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of tensor a (128264) must match the size of tensor b (128256) at non-singleton dimension 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "291it [00:07, 36.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of tensor a (128264) must match the size of tensor b (128256) at non-singleton dimension 0\n",
      "torch.Size([6979588096])\n",
      "percentage of 0 values in the task vector\n",
      "tensor(0.5994)\n"
     ]
    }
   ],
   "source": [
    "all_deltas = []\n",
    "param_sizes = []\n",
    "sft_params = []  # references to SFT params so we can update them\n",
    "rl_state_dict = model_rl.state_dict()\n",
    "sft_state_dict = model_sft.state_dict()\n",
    "\n",
    "missing_in_rl = [name for name, _ in model_sft.named_parameters() if name not in rl_state_dict]\n",
    "missing_in_sft = [name for name, _ in model_rl.named_parameters() if name not in sft_state_dict]\n",
    "\n",
    "if missing_in_rl or missing_in_sft:\n",
    "    print(\"Missing in RL:\", missing_in_rl)\n",
    "    print(\"Missing in SFT:\", missing_in_sft)\n",
    "\n",
    "num_nonzero_dict = {}\n",
    "with torch.no_grad():\n",
    "    for name_sft, param_sft in tqdm(model_sft.named_parameters()):\n",
    "        try:\n",
    "\n",
    "            delta =  rl_state_dict[name_sft] - sft_state_dict[name_sft]\n",
    "            num_nonzero = (delta != 0).sum().item()\n",
    "\n",
    "            num_nonzero_dict[name_sft] = num_nonzero/delta.numel()\n",
    "\n",
    "            param_sizes.append(delta.numel())\n",
    "            sft_params.append(param_sft)\n",
    "\n",
    "            all_deltas.append(delta.view(-1))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "all_deltas_tensor = torch.cat(all_deltas, dim=0)\n",
    "print(all_deltas_tensor.size())\n",
    "print(\"percentage of 0 values in the task vector\")\n",
    "print((all_deltas_tensor == 0 ).sum() / len(all_deltas_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tolerance = 1e-05 -> Fraction close to zero: 0.6310\n"
     ]
    }
   ],
   "source": [
    "tolerances = [1e-5]\n",
    "zero_tensor = torch.zeros_like(all_deltas_tensor)\n",
    "\n",
    "for tol in tolerances:\n",
    "    fraction_close_to_zero = torch.isclose(all_deltas_tensor, zero_tensor, atol=tol).sum() / all_deltas_tensor.numel()\n",
    "    print(f\"Tolerance = {tol:.0e} -> Fraction close to zero: {fraction_close_to_zero:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerwise_sparsity = {}\n",
    "for key in num_nonzero_dict:\n",
    "    if key.startswith('model.layers'):\n",
    "        layer = key.split(\".\")[2]\n",
    "        \n",
    "        if layer not in layerwise_sparsity:\n",
    "            layerwise_sparsity[layer] = []\n",
    "            layerwise_sparsity[layer].append(num_nonzero_dict[key])\n",
    "        else:\n",
    "            layerwise_sparsity[layer].append(num_nonzero_dict[key])\n",
    "\n",
    "\n",
    "layerwise_sparsity = {k:sum(layerwise_sparsity[k])/len(layerwise_sparsity[k]) for k in layerwise_sparsity}\n",
    "layerwise_sparsity = [layerwise_sparsity[k] for k in layerwise_sparsity]\n",
    "keys = [i for i in range(len(layerwise_sparsity))]\n",
    "# plt.bar(keys, layerwise_sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_state_dict['model.layers.0.self_attn.q_proj.weight'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nonzero_dict[\"model.embed_tokens.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in num_nonzero_dict:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment: checking if the same subnetwork is updating across steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "path_to_folder = \"/home/sagnikm3/PRM/outputs_batchsize8\"\n",
    "\n",
    "\n",
    "directories = [os.path.join(path_to_folder, d) for d in os.listdir(path_to_folder) if os.path.isdir(os.path.join(path_to_folder, d))]\n",
    "per_step_deltas = []\n",
    "sparsity = []\n",
    "for i in tqdm(range(len(directories)-1)):\n",
    "    SFT_MODEL = directories[i]\n",
    "    RL_MODEL  = directories[i+1]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL)\n",
    "    model_sft = AutoModelForCausalLM.from_pretrained(\n",
    "        SFT_MODEL,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"cpu\",\n",
    "        cache_dir=\"/shared/storage-01/huggingface/models/\"\n",
    "    )\n",
    "    model_rl = AutoModelForCausalLM.from_pretrained(\n",
    "        RL_MODEL,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"cpu\",\n",
    "        cache_dir=\"/shared/storage-01/huggingface/models/\"\n",
    "    )\n",
    "\n",
    "    all_deltas = []\n",
    "    param_sizes = []\n",
    "    sft_params = []  # references to SFT params so we can update them\n",
    "\n",
    "    num_nonzero_dict = {}\n",
    "    with torch.no_grad():\n",
    "        rl_state_dict = model_rl.state_dict()\n",
    "\n",
    "        for name_sft, param_sft in tqdm(model_sft.named_parameters()):\n",
    "            param_rl = rl_state_dict[name_sft].to(param_sft.device)\n",
    "\n",
    "            delta =  param_rl - param_sft.data\n",
    "\n",
    "            num_nonzero = (delta != 0).sum().item()\n",
    "            num_nonzero_dict[name_sft] = num_nonzero/delta.numel()\n",
    "\n",
    "            param_sizes.append(delta.numel())\n",
    "            sft_params.append(param_sft)\n",
    "\n",
    "            all_deltas.append(delta.view(-1))\n",
    "\n",
    "    all_deltas_tensor = torch.cat(all_deltas, dim=0)\n",
    "    per_step_deltas.append(all_deltas_tensor)\n",
    "    sparsity.append(((all_deltas_tensor ==0 ).sum() / len(all_deltas_tensor)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = []\n",
    "for i in tqdm(range(len(per_step_deltas)-1)):\n",
    "    A = per_step_deltas[i]\n",
    "    B = per_step_deltas[i+1]\n",
    "\n",
    "    A_mask = (A != 0)\n",
    "    B_mask = (B != 0)\n",
    "\n",
    "    overlap_mask = A_mask & B_mask\n",
    "\n",
    "    overlap_count = overlap_mask.sum()\n",
    "    overlap.append(overlap_count/(A!=0).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "sft_state_dict = torch.load('/home/sagnikm3/direct-preference-optimization/.cache/sagnikm3/sft_llama_full_precision/step-119808/policy.pt')['state']\n",
    "rl_state_dict = torch.load('/home/sagnikm3/direct-preference-optimization/.cache/sagnikm3/dp_llama_bf16_2025-02-22_14-53-10_470298/step-20000/policy.pt')['state']\n",
    "\n",
    "sft_state_dict = {k: v.half() for k, v in sft_state_dict.items()}\n",
    "rl_state_dict = {k: v.half() for k, v in rl_state_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_deltas = []\n",
    "param_sizes = []\n",
    "sft_params = []  # references to SFT params so we can update them\n",
    "\n",
    "num_nonzero_dict = {}\n",
    "with torch.no_grad():\n",
    "\n",
    "    for name_sft in tqdm(sft_state_dict):\n",
    "        param_sft = sft_state_dict[name_sft]\n",
    "\n",
    "        param_rl = rl_state_dict[name_sft].to(param_sft.device)\n",
    "        try:\n",
    "\n",
    "            delta =  param_rl - param_sft\n",
    "        \n",
    "\n",
    "            num_nonzero = (delta != 0).sum().item()\n",
    "            num_nonzero_dict[name_sft] = num_nonzero/delta.numel()\n",
    "\n",
    "            param_sizes.append(delta.numel())\n",
    "            sft_params.append(param_sft)\n",
    "\n",
    "            all_deltas.append(delta.view(-1))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "all_deltas_tensor = torch.cat(all_deltas, dim=0)\n",
    "print(all_deltas_tensor.size())\n",
    "print(\"percentage of 0 values in the task vector\")\n",
    "print((all_deltas_tensor ==0 ).sum() / len(all_deltas_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name_sft in tqdm(sft_state_dict):\n",
    "    print(name_sft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_state_dict['model.layers.0.self_attn.q_proj.weight'][0][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_state_dict['model.layers.0.self_attn.q_proj.weight'][0][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "SFT_MODEL = \"deepseek-ai/DeepSeek-V3-Base\"\n",
    "RL_MODEL  = \"deepseek-ai/DeepSeek-R1-Zero\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL)\n",
    "\n",
    "model_sft = AutoModelForCausalLM.from_pretrained(\n",
    "    SFT_MODEL,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"cpu\",\n",
    "    cache_dir=\"/shared/storage-01/huggingface/models/\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "model_rl = AutoModelForCausalLM.from_pretrained(\n",
    "    RL_MODEL,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"cpu\",\n",
    "    cache_dir=\"/shared/storage-01/huggingface/models/\",\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_deltas = []\n",
    "param_sizes = []\n",
    "sft_params = []  # references to SFT params so we can update them\n",
    "\n",
    "num_nonzero_dict = {}\n",
    "with torch.no_grad():\n",
    "    rl_state_dict = model_rl.state_dict()\n",
    "\n",
    "    for name_sft, param_sft in tqdm(model_sft.named_parameters()):\n",
    "        param_rl = rl_state_dict[name_sft].to(param_sft.device)\n",
    "        try:\n",
    "\n",
    "            delta =  param_rl - param_sft.data\n",
    "\n",
    "            num_nonzero = (delta != 0).sum().item()\n",
    "            num_nonzero_dict[name_sft] = num_nonzero/delta.numel()\n",
    "\n",
    "            param_sizes.append(delta.numel())\n",
    "            sft_params.append(param_sft)\n",
    "\n",
    "            all_deltas.append(delta.view(-1))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "all_deltas_tensor = torch.cat(all_deltas, dim=0)\n",
    "print(all_deltas_tensor.size())\n",
    "print(\"percentage of 0 values in the task vector\")\n",
    "print((all_deltas_tensor ==0 ).sum() / len(all_deltas_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SFT_MODEL = \"allenai/Llama-3.1-Tulu-3-8B-SFT\"\n",
    "RL_MODEL  = \"/shared/storage-01/users/sagnikm3/tulu_bs32/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL)\n",
    "\n",
    "model_sft = AutoModelForCausalLM.from_pretrained(\n",
    "    SFT_MODEL,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"cpu\",\n",
    "    cache_dir=\"/shared/storage-01/huggingface/models/\"\n",
    ")\n",
    "model_rl = AutoModelForCausalLM.from_pretrained(\n",
    "    RL_MODEL,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"cpu\",\n",
    "    cache_dir=\"/shared/storage-01/huggingface/models/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.setrecursionlimit(100000)  # If you have very large models, you may need a higher recursion limit\n",
    "\n",
    "import deepspeed.runtime.fp16.loss_scaler\n",
    "import deepspeed.runtime.zero.config\n",
    "\n",
    "from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint\n",
    "\n",
    "# If you haven't already created your `model_sft`, do so here\n",
    "# e.g. model_sft = ...\n",
    "\n",
    "torch.serialization.add_safe_globals([\n",
    "    deepspeed.runtime.zero.config.ZeroStageEnum,\n",
    "    deepspeed.runtime.fp16.loss_scaler.LossScaler,\n",
    "])\n",
    "\n",
    "\n",
    "checkpoint_dir = \"/shared/storage-01/users/sagnikm3/tulu_bs32_epoch5/step_18000/\"\n",
    "\n",
    "state_dict = load_state_dict_from_zero_checkpoint(model_rl, checkpoint_dir)\n",
    "# model_sft.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_deltas = []\n",
    "param_sizes = []\n",
    "sft_params = []  # references to SFT params so we can update them\n",
    "\n",
    "num_nonzero_dict = {}\n",
    "with torch.no_grad():\n",
    "    rl_state_dict = model_rl.state_dict()\n",
    "\n",
    "    for name_sft, param_sft in tqdm(model_sft.named_parameters()):\n",
    "        param_rl = rl_state_dict[name_sft].to(param_sft.device)\n",
    "        try:\n",
    "\n",
    "            delta =  param_rl - param_sft.data\n",
    "\n",
    "            num_nonzero = (delta != 0).sum().item()\n",
    "            num_nonzero_dict[name_sft] = num_nonzero/delta.numel()\n",
    "\n",
    "            param_sizes.append(delta.numel())\n",
    "            sft_params.append(param_sft)\n",
    "\n",
    "            all_deltas.append(delta.view(-1))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "all_deltas_tensor = torch.cat(all_deltas, dim=0)\n",
    "print(all_deltas_tensor.size())\n",
    "print(\"percentage of 0 values in the task vector\")\n",
    "print((all_deltas_tensor ==0 ).sum() / len(all_deltas_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_deltas = []\n",
    "param_sizes = []\n",
    "sft_params = []  # references to SFT params so we can update them\n",
    "rl_state_dict = model_rl.state_dict()\n",
    "sft_state_dict = model_sft.state_dict()\n",
    "\n",
    "for name_sft, param_sft in tqdm(model_sft.named_parameters()):\n",
    "    if name_sft not in rl_state_dict:\n",
    "        print(\"woops\")\n",
    "for name_rl, param_rl in tqdm(model_rl.named_parameters()):\n",
    "    if name_rl not in sft_state_dict:\n",
    "        print(\"woops\")\n",
    "\n",
    "nonzero_dict = {}\n",
    "with torch.no_grad():\n",
    "    for name_sft, param_sft in tqdm(model_sft.named_parameters()):\n",
    "        param_rl = rl_state_dict[name_sft].to(param_sft.device)\n",
    "        try:\n",
    "            delta =  param_rl - param_sft.data\n",
    "            nonzero_dict[name_sft] = delta\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in nonzero_dict.keys():\n",
    "    if len(nonzero_dict[key].shape) >=2:\n",
    "        print(key, nonzero_dict[key].shape)\n",
    "        print(torch.linalg.matrix_rank(nonzero_dict[key].float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_dict[key].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "mask_cpu = torch.load(\"/home/sagnikm3/open-instruct/sft_vs_rl_mask.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_true   = sum(mask.sum().item() for mask in mask_cpu.values())\n",
    "total_elems  = sum(mask.numel()    for mask in mask_cpu.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flat = torch.zeros(total_elems, dtype=torch.bool)\n",
    "perm = torch.randperm(total_elems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat[perm[:total_true]] = True\n",
    "random_global_masks = {}\n",
    "offset = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, mask in mask_cpu.items():\n",
    "    n = mask.numel()\n",
    "    chunk = flat[offset:offset + n]\n",
    "    random_global_masks[name] = chunk.view(mask.shape)\n",
    "    offset += n\n",
    "mask_cpu = random_global_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_count  = sum(m.sum().item()   for m in random_global_masks.values())\n",
    "total_count = sum(m.numel()        for m in random_global_masks.values())\n",
    "\n",
    "density  = true_count / total_count\n",
    "sparsity = 1.0 - density\n",
    "\n",
    "print(f\"Random global mask: {true_count}/{total_count} ones  \"\n",
    "    f\"({density*100:.2f}% density, {sparsity*100:.2f}% sparsity)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verl_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
